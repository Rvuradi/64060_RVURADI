---
title: "Assignment - 5"
author: "ROHIT VURADI"
date: "2023-11-30"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Importing required libraries

```{r}
library(cluster)
library(caret)
library(dendextend)
library(knitr)
library(factoextra)

#Importing the cereals dataset
Cereals_Data<- read.csv("/Users/machd/Downloads/Cereals.csv")

Data_cereals <- data.frame(Cereals_Data[,4:16])

#Removing the missing values from the data
Data_cereals <- na.omit(Data_cereals)


##Data normalization and data scaling
cereals_normalization <- scale(Data_cereals)


#Applying hierarchical clustering to the data using  euclidean distance to normalize measurements
Distance <- dist(cereals_normalization, method = "euclidean")
hierarchical.clustering_complete <- hclust(Distance, method = "complete")


#plotting the dendogram
plot(hierarchical.clustering_complete, cex = 0.7, hang = -1)


##Using agnes() function to perform clustering with single, complete,
#average, ward linkage.

hierarchical.clustering_single <- agnes(cereals_normalization, method = "single")
hierarchical.clustering_complete <- agnes(cereals_normalization, method = "complete")
hierarchical.clustering_average <- agnes(cereals_normalization, method = "average")
hierarchical.clustering_ward <- agnes(cereals_normalization, method = "ward")


##Compare the agglomerative coefficients for single,complete,average and ward.
print(hierarchical.clustering_single$ac)
print(hierarchical.clustering_complete$ac)
print(hierarchical.clustering_average$ac)
print(hierarchical.clustering_ward$ac)


#From the above output the best value we got is 0.904. Plotting the agnes using ward method
#and cutting the Dendrogram. we will take k =4 by noticing the distance.

#2. Choosing the clusters

pltree(hierarchical.clustering_ward, cex = 0.7, hang = -1, main = "Dendrogram of agnes (Using Ward)")
rect.hclust(hierarchical.clustering_ward, k = 5, border = 1:4)
Cluster1 <- cutree(hierarchical.clustering_ward, k=5)
dataframe2 <- as.data.frame(cbind(cereals_normalization,Cluster1))

#We will choose 5 clusters after observing the distance.

#Creating Partitions
set.seed(123)
Partition1 <- Data_cereals[1:50,]
Partition2 <- Data_cereals[51:74,]


#Performing hierarchical Clustering,consedering k = 5.
AG_single <- agnes(scale(Partition1), method = "single")

AG_complete <- agnes(scale(Partition1), method = "complete")

AG_average <- agnes(scale(Partition1), method = "average")

AG_ward <- agnes(scale(Partition1), method = "ward")

cbind(single=AG_single$ac , complete=AG_complete$ac , average= AG_average$ac , ward= AG_ward$ac)
pltree(AG_ward, cex = 0.6, hang = -1, main = "Dendogram of Agnes with Partitioned Data (Using Ward)")
rect.hclust(AG_ward, k = 5, border = 1:4)

cut_2 <- cutree(AG_ward, k = 5)


#Calculating the centeroids

result <- as.data.frame(cbind(Partition1, cut_2))
result[result$cut_2==1,]

centroid_1 <- colMeans(result[result$cut_2==1,])
result[result$cut_2==2,]

centroid_2 <- colMeans(result[result$cut_2==2,])
result[result$cut_2==3,]

centroid_3 <- colMeans(result[result$cut_2==3,])
result[result$cut_2==4,]

centroid_4 <- colMeans(result[result$cut_2==4,])

centroids <- rbind(centroid_1, centroid_2, centroid_3, centroid_4)
x2 <- as.data.frame(rbind(centroids[,-14], Partition2))

#Calculating the Distance

Distance_1 <- get_dist(x2)
Matrix_1 <- as.matrix(Distance_1)

dataframe1 <- data.frame(data=seq(1,nrow(Partition2),1), Clusters = rep(0,nrow(Partition2)))
for(i in 1:nrow(Partition2)) 
{dataframe1[i,2] <- which.min(Matrix_1[i+4, 1:4])}
dataframe1

cbind(dataframe2$Cluster1[51:74], dataframe1$Clusters)
table(dataframe2$Cluster1[51:74] == dataframe1$Clusters)


#We can say that the model is partially stable as we are getting 12 FALSE and 12 TRUE 
#3) The elementary public schools would like to choose a set of Cereals_Data to 
#include in their daily cafeterias. Every day a different cereal is offered, 
#but all Cereals_Data should support a healthy diet. For this goal, you are requested to find a cluster of “healthy #Cereals_Data.”

#Clustering Healthy Cereals_Data.
Healthy_Cereals <- Cereals_Data
Healthy_Cereals_new <- na.omit(Healthy_Cereals)
HealthyClust <- cbind(Healthy_Cereals_new, Cluster1)
HealthyClust[HealthyClust$Cluster1==1,]
HealthyClust[HealthyClust$Cluster1==2,]
HealthyClust[HealthyClust$Cluster1==3,]
HealthyClust[HealthyClust$Cluster1==4,]

#Mean ratings to determine the best cluster.
mean(HealthyClust[HealthyClust$Cluster1==1,"rating"])
mean(HealthyClust[HealthyClust$Cluster1==2,"rating"])
mean(HealthyClust[HealthyClust$Cluster1==3,"rating"])
mean(HealthyClust[HealthyClust$Cluster1==4,"rating"])

#Since mean ratings are highest for cluster 1 as 73.84446, we can consider cluster 1.
```